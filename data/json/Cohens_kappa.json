{"cursor":"18157","size":15,"audio":[],"currentlang":"en","article":"'Cohen's kappa coefficient' is a statistical measure of inter-rater agreement or\ninter-annotator agreement Carletta, Jean. (1996) [http://acl.ldc.upenn.edu/J/J96/J96-\n2004.pdf Assessing agreement on classification tasks: The kappa statistic.]\nComputational Linguistics, 22(2), pp. 249â254. for qualitative\n(categorical) items. It is generally thought to be a more robust measure than\nsimple percent agreement calculation since Îº takes into account the agreement\noccurring by chance. Some researchers have expressed\nconcern over Îº's tendency to take the observed categories' frequencies as\ngivens, which can have the effect of underestimating agreement for a category\nthat is also commonly used; for this reason, Îº is considered an overly\nconservative measure of agreement.\n\nOthers contest the assertion that kappa \"takes into account\" chance\nagreement. To do this effectively would require an explicit model of how chance\naffects rater decisions. The so-called chance adjustment of kappa statistics\nsupposes that, when not completely certain, raters simply guessâa very\nunrealistic scenario.\n","linknr":231,"url":"Cohen's_kappa","recorded":1362480355,"links":27,"instances":["statistician"],"pdf":["http://acl.ldc.upenn.edu/J/J96/J96-2004.pdf","http://www.na-mic.org/Wiki/images/d/df/Kapp_and_decision_making_models.pdf","http://www.na-mic.org/Wiki/images/d/df/Kapp_and_decision_making_models.pdf","http://agreestat.com/research_papers/inter_rater_reliability_dependency.pdf","http://agreestat.com/research_papers/inter_rater_reliability_dependency.pdf","http://www.agreestat.com/research_papers/bjmsp2008_interrater.pdf","http://www.agreestat.com/research_papers/bjmsp2008_interrater.pdf","http://www.agreestat.com/research_papers/psychometrika2008_irr_random_raters.pdf","http://www.agreestat.com/research_papers/psychometrika2008_irr_random_raters.pdf","http://www.agreestat.com/research_papers/wiley_encyclopedia2008_eoct631.pdf","http://dl.dropbox.com/u/27743223/201209-eacl2012-Kappa.pdf"],"categories":["Categorical data","Non-parametric statistics","Inter-rater reliability"],"headings":["Calculation","Example","Same percentages but different numbers","Significance and magnitude","Weighted kappa","Kappa maximum","See also","References","External links"],"image":["//upload.wikimedia.org/math/1/5/7/157b026a8dd5b4f05dac41e669215e4f.png","//upload.wikimedia.org/math/d/f/f/dff42112abdab858500baa4a5b3800f3.png","//upload.wikimedia.org/math/5/9/9/599173231b6b7cbdea05e3002dc6b52c.png","//upload.wikimedia.org/math/f/6/b/f6b49bf92f99f6be97f578efaec8c975.png","//upload.wikimedia.org/math/7/c/3/7c394551fdfeb10412822265c3fac709.png","//upload.wikimedia.org/math/d/4/c/d4ceb969c50979e2dfbb7fe3e83d7a24.png","//upload.wikimedia.org/math/a/b/c/abca360236b75caf68f35f92c33284d8.png","//upload.wikimedia.org/math/0/c/2/0c2e3fe412f355159a3270227a95808c.png","//upload.wikimedia.org/math/2/d/b/2dbc0665e308fabc49e2118c3869a6ec.png","//upload.wikimedia.org/math/b/c/9/bc9594a5c53bdc265c2f5af1f45cff82.png","//upload.wikimedia.org/math/7/9/d/79d001baa5d23bfe6319870a52234e15.png","//upload.wikimedia.org/math/6/5/5/6556f0882b84a784c3dd158677b5ce84.png","//upload.wikimedia.org/math/5/3/9/53906497f56478603701d3a82a926261.png","//bits.wikimedia.org/static-1.21wmf10/skins/vector/images/search-ltr.png?303-4","//bits.wikimedia.org/images/wikimedia-button.png","//bits.wikimedia.org/static-1.21wmf10/skins/common/images/poweredby_mediawiki_88x31.png"],"tags":[["jacob_cohen","statistician"]],"members":["jacob_cohen"],"related":["Statistical","Inter-rater_agreement","Jacob_Cohen_(statistician)","Scott's_Pi","Scott's_Pi","Fleiss'_kappa","Joseph_L._Fleiss","Scott's_Pi","Fleiss'_kappa","Intraclass_correlation"]}