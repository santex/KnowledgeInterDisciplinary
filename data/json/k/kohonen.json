{"micrownet":[],"duck":["kohonen","Software description: kohonen Self Organizing Maps algorithm implementation in python, with other machine learning algorithms for comparison (kmeans, knn, svm, etc) (Python)."],"common":{"milestones":[],"image":[[],[]]},"Lists":[],"created":1373524588,"book":[],"micro-www":{"kohonen":[""]},"wiki":{"cat":["Neural networks","Dimension reduction","Data clustering algorithms"],"text":"\n\nA 'self-organizing map' ('SOM') or 'self-organizing feature map' ('SOFM') is a\ntype of artificial neural network (ANN) that is trained using unsupervised\nlearning to produce a low-dimensional (typically two-dimensional), discretized\nrepresentation of the input space of the training samples, called a 'map'. Self-\norganizing maps are different from other artificial neural networks in the sense\nthat they use a neighborhood function to preserve the topological properties of\nthe input space.\n\n voting patterns visualized in Synapse. The first two boxes show clustering and\n distances while the remaining ones show the component planes. Red means a yes\n vote while blue means a no vote in the component planes (except the party\n component where red is Republican and blue is Democratic).]]<!-- --> This makes\n SOMs useful for visualizing low-dimensional views of high-dimensional data,\n akin to multidimensional scaling. The model was first described as an\n artificial neural network by the Finnish professor Teuvo Kohonen, and is\n sometimes called a 'Kohonen map' or 'network'.<ref\n name=\"KohonenMap\"></ref><ref></ref>\n\nLike most artificial neural networks, SOMs operate in two modes: training and\nmapping. \"Training\" builds the map using input examples (a competitive process,\nalso called vector quantization), while \"mapping\" automatically classifies a new\ninput vector.\n\nA self-organizing map consists of components called nodes or neurons. Associated\nwith each node is a weight vector of the same dimension as the input data\nvectors and a position in the map space. The usual arrangement of nodes is a two-\ndimensional regular spacing in a hexagonal or rectangular grid. The self-\norganizing map describes a mapping from a higher dimensional input space to a\nlower dimensional map space. The procedure for placing a vector from data space\nonto the map is to find the node with the closest (smallest distance metric)\nweight vector to the data space vector.\n\nWhile it is typical to consider this type of network structure as related to\nfeedforward networks where the nodes are visualized as being attached, this type\nof architecture is fundamentally different in arrangement and motivation.\n\nUseful extensions include using toroidal grids where opposite edges are\nconnected and using large numbers of nodes. {{Reference necessary |text= It has\nbeen shown that while self-organizing maps with a small number of nodes behave\nin a way that is similar to K-means, larger self-organizing maps rearrange data\nin a way that is fundamentally topological in character. |date=September 2012}}\n\nIt is also common to use the U-Matrix.<ref name=\"UltschSiemon1990\"></ref> The\nU-Matrix value of a particular node is the average distance between the node and\nits closest neighbors.<ref name=\"Ultsch2003\" /> In a square grid, for instance,\nwe might consider the closest 4 or 8 nodes (the Von Neumann and Moore\nneighborhoods, respectively), or six nodes in a hexagonal grid.\n\nLarge SOMs display emergent properties. In maps consisting of thousands of\nnodes, it is possible to perform cluster operations on the map itself.<ref\nname=\"Ultsch2007\"></ref>\n","title":"Self-organizing%20map","headings":["Learning algorithm","Interpretation","Alternatives","See also","References","External links"]},"micro-relation":["3: Euclidean_distance","2: Competitive_learning","2: U-Matrix","1: Artificial_neural_network","1: Unsupervised_learning","1: Topology","1: Peltarion_Synapse","1: Scientific_visualization","1: Multidimensional_scaling","1: Finland","1: Teuvo_Kohonen","1: Vector_quantization","1: Hexagonal","1: Rectangular","1: Feedforward_neural_networks","1: Torus","1: K-means_algorithm","1: Von_Neumann_neighborhood","1: Moore_neighborhood","1: Sense","1: Cerebral_cortex","1: Human_brain","1: Principal_component","1: Eigenvectors","1: Monotonically_decreasing","1: Bootstrap_sampling","1: Gaussian_function","1: Iris_flower_data_set","1: Fraction_of_variance_unexplained","1: Principal_components_analysis","1: Elastic_map","1: Nonlinear_dimensionality_reduction#Principal_curves_and_manifolds","1: International_Journal_of_Neural_Systems","1: Spline_interpolation","1: Elastic_energy","1: Least_squares","1: Approximation_error","1: Neural_gas","1: Hybrid_Kohonen_SOM"]}