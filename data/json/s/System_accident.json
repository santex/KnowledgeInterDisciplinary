{"cursor":"13459","size":15,"audio":[],"currentlang":"en","article":"A 'system accident', also called a \"normal accident\", is an \"unanticipated\ninteraction of multiple failures\" in a complex system. This complexity can\neither be technological or organizational, and often is both. Perrow, Charles (1984). [http://books.google.com/books?id=VC5hYoM-\nw4N0C&printsec=frontcover&dq=Charles+Perrow#PPA10,M1 Normal Accidents: Living\nwith High-Risk Technologies, With a New Afterword and a Postscript on the Y2K\nProblem], Princeton, New Jersey: Princeton University Press, ISBN 0-691-00412-9,\n1984, 1999 (first published by Basic Books 1984) A system accident can be\nvery easy to see in hindsight, but very difficult to see in foresight. Ahead of\ntime, there are simply too many possible action pathways to seriously consider\nall of them.\n\nThese accidents often resemble Rube Goldberg devices in the way that small\nerrors of judgment, flaws in technology, and insignificant damages combine to\nform an emergent disaster. System accidents were described in 1984 by Charles\nPerrow, who termed them \"normal accidents\", as having two main characteristics:\ninteractive complexity and tight coupling. James T. Reason extended this\napproach with human reliability and the Swiss cheese model, now\nwidely accepted in aviation safety and healthcare.\n\nOnce an enterprise passes a certain point in size, with many employees,\nspecialization, backup systems, double-checking, detailed manuals, and formal\ncommunication, employees can all too easily recourse to protocol, habit, and\n\"being right.\" Rather like attempting to watch a complicated movie in a language\none is unfamiliar with, the narrative thread of what is going on can be lost.\nAnd other phenomena, such as groupthink, can also be occurring at the same time,\nfor real-world accidents of course almost always have multiple causes, and not\njust the single cause that could have prevented the accident at the very last\nminute. In particular, it is a mark of a dysfunctional organization to simply\nblame the last person who touched something.\n\nThe processes of formalized organizations are often largely opaque. Perrow call\nthis \"incomprehensibility.\"\n\nThere is an aspect of an animal devouring its tail, in that more formality and\neffort to get it just right can actually make the situation worse. Langewiesche, William (March 1998). [http://www.theatlantic.com/magazine/archive/1998/03/the-lessons-of-valujet-\n592/6534/4/ The Lessons of Valujet 592], The Atlantic. See especially the last\nthree paragraphs of this four-part article: â . . . Understanding why might keep\nus from making the system even more complex, and therefore perhaps more\ndangerous, too.â For example, the more organizational rigmarole involved\nin adjusting to changing conditions, the more employees will delay reporting\nchanging conditions. And the more emphasis on formality, the less likely it is\nthat employees and managers will engage in real communication. And new rules can\nactually make it worse, both by adding a new additional layer of complexity and\nby telling employees once again that they are not to think, but are instead\nsimply to mechanically follow rules.\n\nRegarding the May 1996 crash of Valujet (AirTran) in the Florida Everglades,\nWilliam Langewiesche writes, \"Such pretend realities extend even into the most\nself-consciously progressive large organizations, with their attempts to\nformalize informality, to deregulate the workplace, to share profits and\nresponsibilities, to respect the integrity and initiative of the individual. The\nsystems work in principle, and usually in practice as well, but the two may have\nlittle to do with each other. Paperwork floats free of the ground and obscures\nthe murky workplaces where, in the confusion of real life, system accidents are\nborn.\"\n","linknr":163,"url":"System_accident","recorded":1362485868,"links":16,"instances":[],"pdf":["http://history.nasa.gov/ap13rb/ch5.pdf","http://regnet.anu.edu.au/program/review/Publications/HopkinsP3.pdf","http://regnet.anu.edu.au/program/review/Publications/HopkinsP3.pdf"],"categories":["Systems engineering","Safety engineering","Failure"],"headings":["Possible system accidents","References"],"image":["//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png","//upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Ambox_question.svg/40px-Ambox_question.svg.png","//bits.wikimedia.org/static-1.21wmf10/skins/vector/images/search-ltr.png?303-4","//bits.wikimedia.org/images/wikimedia-button.png","//bits.wikimedia.org/static-1.21wmf10/skins/common/images/poweredby_mediawiki_88x31.png"],"tags":[],"members":[],"related":["Normal_Accidents","Complex_system","Rube_Goldberg_device","Emergence","Charles_Perrow","Interactive_complexity","Tight_coupling","Human_reliability","Swiss_Cheese_Model","Aviation_safety","Three_Mile_Island_accident","Nuclear_accident"]}