{"cursor":"4198","size":15,"audio":[],"currentlang":"en","article":"\nThe 'accuracy paradox' for predictive analytics states that predictive models\nwith a given level of accuracy may have greater predictive power than models\nwith higher accuracy. It may be better to avoid the accuracy metric in favor of\nother metrics such as precision and recall.\n\nAccuracy is often the starting point for analyzing the quality of a predictive\nmodel, as well as an obvious criterion for prediction. Accuracy measures the\nratio of correct predictions to the total number of cases evaluated. It may seem\nobvious that the ratio of correct predictions to cases should be a key metric. A\npredictive model may have high accuracy, but be useless.\n\nIn an example predictive model for an insurance fraud application, all cases\nthat are predicted as high-risk by the model will be investigated. To evaluate\nthe performance of the model, the insurance company has created a sample data\nset of 10,000 claims. All 10,000 cases in the validation sample have been\ncarefully checked and it is known which cases are fraudulent. To analyze the\nquality of the model, the insurance uses the table of confusion. The definition\nof accuracy, the table of confusion for model M 1Fraud ,\nand the calculation of accuracy for model M 1Fraud is\nshown below.\n\n\\mathrm{A}(M) = \\frac{TN + TP}{TN + FP + FN + TP} where\n: TN is the number of true negative cases FP is the number of false positive\n: cases FN is the number of false negative cases TP is the number of true\n: positive cases\n\nFormula 1: Definition of Accuracy\n\n{| class=\"wikitable\"\n!\n!Predicted Negative !Predicted Positive\n|-\n|Negative Cases||9,700||150\n|-\n|Positive Cases||50||100\n|}\n\nTable 1: Table of Confusion for Fraud Model M 1Fraud .\n\n\\mathrm A (M) = \\frac{9,700 + 100}{9,700 + 150 + 50 + 100} = 98.0%\n\nFormula 2: Accuracy for model M 1Fraud\n\nWith an accuracy of 98.0% model M 1Fraud appears to perform\nfairly well. The paradox lies in the fact that accuracy can be easily improved\nto 98.5% by always predicting \"no fraud\". The table of confusion and the\naccuracy for this trivial âalways predict negativeâ model\nM 2Fraud and the accuracy of this model are shown below.\n\n{| class=\"wikitable\"\n!\n!Predicted Negative !Predicted Positive\n|-\n|Negative Cases||9,850||0\n|-\n|Positive Cases||150||0\n|}\n\nTable 2: Table of Confusion for Fraud Model M 2Fraud .\n\n\\mathrm{A}(M) = \\frac{9,850 + 0}{9,850 + 150 + 0 + 0} = 98.5%\n\nFormula 3: Accuracy for model M 2Fraud\n\nModel M 2Fraud reduces the rate of inaccurate predictions\nfrom 2% to 1.5%. This is an apparent improvement of 25%. The new model\nM 2Fraud shows fewer incorrect predictions and markedly\nimproved accuracy, as compared to the original model\nM 1Fraud , but is obviously useless.\n\nThe alternative model M 2Fraud does not offer any value to\nthe company for preventing fraud. The less accurate model is more useful than\nthe more accurate model.\n\nModel improvements should not be measured in terms of accuracy gains. It may be\ngoing too far to say that accuracy is irrelevant, but caution is advised when\nusing accuracy in the evaluation of predictive models.\n","linknr":96,"url":"Accuracy_paradox","recorded":1362480351,"links":13,"instances":["statistics","information_retrieval"],"pdf":["http://www.utwente.nl/ewi/trese/graduation_projects/2009/Abma.pdf"],"categories":["Statistical paradoxes","Machine learning","Data mining"],"headings":["See also","Bibliography"],"image":["//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png","//upload.wikimedia.org/math/b/9/a/b9aa2ca868f9a20744b7aff1a14e063c.png","//upload.wikimedia.org/math/d/d/6/dd60d162b4a33a5d368139049cce661a.png","//upload.wikimedia.org/math/7/1/a/71ac44d8f7f5a554c0e156f1fc630ef3.png","//bits.wikimedia.org/static-1.21wmf10/skins/vector/images/search-ltr.png?303-4","//bits.wikimedia.org/images/wikimedia-button.png","//bits.wikimedia.org/static-1.21wmf10/skins/common/images/poweredby_mediawiki_88x31.png"],"tags":[["recall","information_retrieval"],["validity","statistics"]],"members":["validity","recall"],"related":["Predictive_analytics","Accuracy","Predictive_power","Accuracy_and_precision","Recall_(information_retrieval)","Insurance_fraud","Validity_(statistics)","Table_of_confusion","Receiver_operating_characteristic"]}