{"cursor":"39281","size":15,"audio":[],"currentlang":"en","article":"\n\n'Information theory' is a branch of applied mathematics, electrical engineering,\nbioinformatics, and computer science involving the quantification of\ninformation. Information theory was developed by Claude E. Shannon to find\nfundamental limits on signal processing operations such as compressing data and\non reliably storing and communicating data. Since its inception it has broadened\nto find applications in many other areas, including statistical inference,\nnatural language processing, cryptography, neurobiology, F. Rieke, D.\nWarland, R Ruyter van Steveninck, W Bialek, Spikes: Exploring the Neural Code.\nThe MIT press (1997). the evolution cf. Huelsenbeck, J. P., F.\nRonquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny\nand its impact on evolutionary biology, Science '294':2310-2314 and\nfunction Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip\nSmallwood, Jeremy Nathans, Peter K. Rogan, [http://alum.mit.edu/www/toms/ Thomas\nD. Schneider], Michael Dean (1998) Organization of the ABCR gene: analysis of\npromoter and splice junction sequences, Gene '215':1, 111-122 of molecular\ncodes, model selection Burnham, K. P. and Anderson D. R. (2002) Model\nSelection and Multimodel Inference: A Practical Information-Theoretic Approach,\nSecond Edition (Springer Science, New York) ISBN 978-0-387-95364-9. in\necology, thermal physics, Jaynes, E. T. (1957) [http://bayes.wustl.edu/\nInformation Theory and Statistical Mechanics], Phys. Rev. '106':620\nquantum computing, plagiarism detection Charles H. Bennett, Ming Li, and Bin\nMa (2003) [http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTIC-\nLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 Chain Letters and Evolutionary\nHistories], Scientific American '288':6, 76-81 and other forms of data\nanalysis. {{Cite web\n  | author = David R. Anderson title = Some background on why people in the\n  | empirical sciences may want to better understand the information-theoretic\n  | methods date = November 1, 2003 url =\n  | http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf format = pdf\n  | accessdate = 2010-06-23}}\n\n\nA key measure of information is known as entropy, which is usually expressed by\nthe average number of bits needed to store or communicate one symbol in a\nmessage. Entropy quantifies the uncertainty involved in predicting the value of\na random variable. For example, specifying the outcome of a fair coin flip (two\nequally likely outcomes) provides less information (lower entropy) than\nspecifying the outcome from a roll of a (six equally likely outcomes).\n\nApplications of fundamental topics of information theory include lossless data\ncompression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPGs), and\nchannel coding (e.g. for Digital Subscriber Line (DSL)). The field is at the\nintersection of mathematics, statistics, computer science, physics,\nneurobiology, and electrical engineering. Its impact has been crucial to the\nsuccess of the Voyager missions to deep space, the invention of the compact\ndisc, the feasibility of mobile phones, the development of the Internet, the\nstudy of linguistics and of human perception, the understanding of black holes,\nand numerous other fields. Important sub-fields of information theory are source\ncoding, channel coding, algorithmic complexity theory, algorithmic information\ntheory, information-theoretic security, and measures of information.\n","linknr":845,"url":"Information_theory","recorded":1362505079,"links":79,"instances":["scientist","information_theory","information","information_theory","mathematics","communications","cryptography","academic","information_theory","cryptography","cybernetics","data","academic_field","information_gathering","information","file_format","mathematics"],"pdf":["http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf","http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf","http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf","http://www.dotrose.com/etext/90_Miscellaneous/transmission_of_information_1928b.pdf","http://www.racing.saratoga.ny.us/kelly.pdf","http://www.research.ibm.com/journal/rd/441/landauerii.pdf","http://www.nd.edu/~jnl/ee80653/tutorials/sunil.pdf","http://compbio.umbc.edu/Documents/Introduction_Information_Theory.pdf"],"categories":["Communication","Cybernetics","Formal sciences","Information Age","Information theory|*"],"headings":["Overview","Historical background","Quantities of information","Coding theory","Applications to other fields","See also","References","External links"],"image":["//upload.wikimedia.org/math/2/c/0/2c0da064f41942004061ff301efdb84d.png","//upload.wikimedia.org/math/a/6/9/a69518d0af009a76b6da3c9b8a928f13.png","//upload.wikimedia.org/math/9/4/3/943b00a58935c659805c681142033b55.png","//upload.wikimedia.org/math/1/8/9/189c028399d91e0fca282c1955fa491a.png","//upload.wikimedia.org/math/e/9/8/e988c1a14afa5a1522be52ceb54d78bf.png","//upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/200px-Binary_entropy_plot.svg.png","//bits.wikimedia.org/static-1.21wmf10/skins/common/images/magnify-clip.png","//upload.wikimedia.org/math/e/5/e/e5e4f67c8cf506a16dfe36b328b30fed.png","//upload.wikimedia.org/math/c/1/d/c1d9f50f86825a1a2302ec2449c17196.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/0/2/1/0216f55c43d81481da88827bfab7cb01.png","//upload.wikimedia.org/math/7/f/b/7fb1228330457b21cf9120567002c9e4.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/4/1/3/4130c89f2d12c3ac81aba3adbff28685.png","//upload.wikimedia.org/math/9/d/d/9dd4e461268c8034f5c8564e155c67a6.png","//upload.wikimedia.org/math/d/3/2/d32828eafbf0c042aef9f51564009664.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/4/f/0/4f0080f2c78d5b39d6f8ce8dfa076f8e.png","//upload.wikimedia.org/math/4/e/0/4e05e522a4e0e14c7324262acb17d5f4.png","//upload.wikimedia.org/math/b/d/9/bd926867374fa3a191e1e3e9fb0c9cdf.png","//upload.wikimedia.org/math/8/1/2/8128cfb614ebb99bdbf62bea3d96a912.png","//upload.wikimedia.org/math/4/4/1/441779a0035a80913795b9b8933fcb01.png","//upload.wikimedia.org/math/3/1/b/31bd2de8f95a4ef673aff2c53d3afb2b.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/f/0/0/f00acce613318349cb04ab296486fc11.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/f/0/0/f00acce613318349cb04ab296486fc11.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/0/a/2/0a2e66566a03512cb500db06175aa4dd.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/2/0/a/20aacd9fd5d717de99c4656d983c9484.png","//upload.wikimedia.org/math/a/c/f/acfe99276a861ba5fd6e773d298ba555.png","//upload.wikimedia.org/math/0/2/1/02129bb861061d1a052c592e2dc6b383.png","//upload.wikimedia.org/math/5/7/c/57cec4137b614c87cb4e24a3d003a3e0.png","//upload.wikimedia.org/math/3/9/1/391eeed3480f23a5303f87bede98769a.png","//upload.wikimedia.org/math/c/e/7/ce774d9cab3ae0bdf522cd0839bed364.png","//upload.wikimedia.org/math/2/2/2/2220f2cec8caf806599b6083a6255994.png","//upload.wikimedia.org/math/7/2/2/72268ae9e9e089bc0ae3c61b7435f434.png","//upload.wikimedia.org/math/a/7/9/a79044209a8ae6976a20110235bb4e89.png","//upload.wikimedia.org/math/8/c/9/8c9a47120e2cf50d9600cbdfe5f3907f.png","//upload.wikimedia.org/math/9/5/f/95f35b63d6e42c137c3d8b9aa971230d.png","//upload.wikimedia.org/math/b/6/f/b6ff37af1b7f85fadc507dc752c51d96.png","//upload.wikimedia.org/wikipedia/commons/thumb/5/5a/CDSCRATCHES.jpg/220px-CDSCRATCHES.jpg","//bits.wikimedia.org/static-1.21wmf10/skins/common/images/magnify-clip.png","//upload.wikimedia.org/math/a/6/0/a6016a4952afa79ccd5df2f5536e1ed4.png","//upload.wikimedia.org/math/7/d/e/7de69d8cf3be4709f4e30684185fc561.png","//upload.wikimedia.org/wikipedia/commons/thumb/4/48/Comm_Channel.svg/500px-Comm_Channel.svg.png","//upload.wikimedia.org/math/a/e/d/aedea3d9f994342b301191d6f1066ef9.png","//upload.wikimedia.org/math/a/e/d/aedea3d9f994342b301191d6f1066ef9.png","//upload.wikimedia.org/math/5/0/b/50bbd36e1fd2333108437a2ca378be62.png","//upload.wikimedia.org/math/e/8/5/e859dd7349ce63086c89ed113b66bf2f.png","//upload.wikimedia.org/math/5/1/1/51171c411631d7f6a9502daa52cf616b.png","//upload.wikimedia.org/math/e/2/6/e2643d06c8c49297e4d10cce1a9ad486.png","//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Binary_symmetric_channel.svg/156px-Binary_symmetric_channel.svg.png","//upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Binary_erasure_channel.svg/156px-Binary_erasure_channel.svg.png","//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Nuvola_apps_edu_mathematics_blue-p.svg/28px-Nuvola_apps_edu_mathematics_blue-p.svg.png","//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Wikiquote-logo-en.svg/40px-Wikiquote-logo-en.svg.png","//bits.wikimedia.org/static-1.21wmf10/skins/vector/images/search-ltr.png?303-4","//bits.wikimedia.org/images/wikimedia-button.png","//bits.wikimedia.org/static-1.21wmf10/skins/common/images/poweredby_mediawiki_88x31.png"],"tags":[["entropy","information_theory"],["symbol","data"],["zip","file_format"],["rubric","academic"],["informatics","academic_field"],["code","cryptography"],["intelligence","information_gathering"],["ban","information"],["redundancy","information_theory"],["entropy","information_theory"],["channel","communications"],["nat","information"],["metric","mathematics"],["redundancy","information_theory"],["redundancy","information_theory"],["ban","information"],["redundancy","information_theory"],["key","cryptography"],["extractor","mathematics"],["intelligence","information_gathering"],["ban","information"],["channel","communications"],["receiver","information_theory"],["redundancy","information_theory"],["variety","cybernetics"],["david_mackay","scientist"]],"members":["david_mackay","receiver","nat","entropy","metric","channel","key","rubric","redundancy","code","variety","symbol","informatics","intelligence","ban","zip","extractor"],"related":["Applied_mathematics","Electrical_engineering","Bioinformatics","Computer_science","Quantification","Information","Claude_E._Shannon","Signal_processing","Data_compression","Computer_data_storage","Telecommunication","Statistical_inference","Natural_language_processing","Cryptography","Neurobiology","Quantum_computing","Data_analysis","Entropy_(information_theory)","Symbol_(data)","Random_variable","Coin_flip","Lossless_data_compression","ZIP_(file_format)","Lossy_data_compression","MP3","JPG","Channel_capacity","DSL","Mathematics","Statistics","Computer_science","Physics","Neurobiology","Electrical_engineering","Voyager_program","Internet","Linguistics","Black_hole","Source_coding","Channel_coding","Algorithmic_complexity_theory","Algorithmic_information_theory","Information-theoretic_security","Data_compression","Source_coding","Channel_capacity","Claude_Elwood_Shannon","A_Mathematical_Theory_of_Communication","Source_coding_theorem","Information_entropy","Noisy-channel_coding_theorem","Rubric_(academic)","Adaptive_system","Anticipatory_system","Artificial_intelligence","Complex_system","Complexity_science","Cybernetics","Informatics_(academic_field)","Machine_learning","Systems_science","Coding_theory","Data_compression","Error-correction","Code_(cryptography)","Cipher","Cryptography","Cryptanalysis","Information_retrieval","Intelligence_(information_gathering)","Gambling","Statistics","Musical_composition","Claude_E._Shannon","A_Mathematical_Theory_of_Communication","Bell_Labs","Harry_Nyquist","Ralph_Hartley","Ban_(information)","Alan_Turing","Cryptanalysis_of_the_Enigma","Thermodynamics","Ludwig_Boltzmann","J._Willard_Gibbs","Rolf_Landauer","Information_entropy","Redundancy_(information_theory)","Source_coding_theorem","Mutual_information","Channel_capacity","Noisy-channel_coding_theorem","Shannon–Hartley_law","Gaussian_channel","Bit","Probability_theory","Statistics","Entropy_(information_theory)","Random_variable","Mutual_information","Data_compression","Channel_(communications)","Units_of_measurement","Information_entropy","Bit","Binary_logarithm","Nat_(information)","Natural_logarithm","Deciban","Common_logarithm","Self-information","Expected_value","Statistical_independence","Chess","Pointwise_mutual_information","Symmetric_function","Kullback–Leibler_divergence","Posterior_probability","Prior_probability","Likelihood-ratio_test","Multinomial_distribution","Pearson's_chi-squared_test","Probability_distribution","Metric_(mathematics)","Triangle_inequality","Self-information","Surprisal","Rényi_entropy","Differential_entropy","Conditional_mutual_information","Coding_theory","Data_compression","Error_correction","Lossless_data_compression","Lossy_data_compression","Rate–distortion_theory","Redundancy_(information_theory)","Error_correction","Broadcast_channel","Relay_channel","Computer_network","Network_information_theory","Independent_identically_distributed_random_variables","Ergodic_theory","Stationary_process","Stochastic_process","Channel_capacity","Entropy_rate","Redundancy_(information_theory)","Data_compression","Ethernet","Cable","Conditional_probability","Mutual_information","Error_detection_and_correction","Gaussian_noise","Shannon–Hartley_theorem","Binary_symmetric_channel","Binary_entropy_function","Binary_erasure_channel","Cryptography","Cryptanalysis","Turing","Ban_(information)","Ultra","Enigma_machine","Victory_in_Europe_Day","Unicity_distance","Redundancy_(information_theory)","Plaintext","Ciphertext","Brute_force_attack","Public-key_cryptography","Symmetric-key_algorithm","Block_cipher","Information_theoretic_security","One-time_pad","Mutual_information","Plaintext","Ciphertext","Key_(cryptography)","Venona_project","Soviet_Union","Pseudorandom_number_generator","Cryptographically_secure_pseudorandom_number_generator","Random_seed","Extractor_(mathematics)","Min-entropy","Rényi_entropy","Random_variable","Digital_signal_processing","Gambling_and_information_theory","Black_hole_information_paradox","Bioinformatics","Music","Communication_theory","Philosophy_of_information","Cryptanalysis","Cryptography","Cybernetics","Entropy_in_thermodynamics_and_information_theory","Gambling","Intelligence_(information_gathering)","Reflection_seismology","Ralph_Hartley","History_of_information_theory","Claude_Elwood_Shannon","Timeline_of_information_theory","Hubert_Yockey","Coding_theory","Detection_theory","Estimation_theory","Fisher_information","Information_algebra","Information_asymmetry","Information_geometry","Information_theory_and_measure_theory","Kolmogorov_complexity","Logic_of_information","Network_coding","Philosophy_of_Information","Quantum_information_science","Semiotic_information_theory","Source_coding","Ban_(information)","Channel_capacity","Channel_(communications)","Communication_source","Conditional_entropy","Covert_channel","Decoder","Differential_entropy","Encoder","Information_entropy","Joint_entropy","Kullback-Leibler_divergence","Mutual_information","Pointwise_Mutual_Information","Receiver_(information_theory)","Redundancy_(information_theory)","Rényi_entropy","Self-information","Unicity_distance","Variety_(cybernetics)","Claude_Elwood_Shannon","A_Mathematical_Theory_of_Communication","Andrey_Kolmogorov","Claude_E._Shannon","Robert_Gallager","Thomas_M._Cover","Imre_Csiszar","Fazlollah_Reza","James_Gleick","Princeton_University_Press","David_MacKay_(scientist)","Arithmetic_coding","Low-density_parity-check_code","Turbo_code"]}